@inproceedings{Shetty2021,
  title = {Decentralized {{Connectivity Maintenance}} for {{Multi}}-Robot {{Systems Under Motion}} and {{Sensing Uncertainties}}},
  booktitle = {Proceedings of the {{Institute}} of {{Navigation GNSS}}+ Conference},
  author = {Shetty, Akshay and Hussain, Timmy and Gao, Grace},
  year = {2021},
  month = sep,
  pages = {14},
  address = {{St. Louis, Missouri}},
  abstract = {Communication connectivity is desirable for safe and efficient operation of multi-robot systems. While decentralized algorithms for connectivity maintenance have been explored in recent literature, the majority of these works do not account for robot motion and sensing uncertainties. These uncertainties are inherent in practical robots and result in robots deviating from their desired positions which could potentially result in a loss of connectivity. In this paper we present a Decentralized Connectivity Maintenance algorithm accounting for robot motion and sensing Uncertainties (DCMU). We first propose a novel weighted graph definition for the multi-robot system that accounts for the aforementioned uncertainties along with realistic connectivity constraints such as line-of-sight connectivity and collision avoidance. Next we design a decentralized gradient-based controller for connectivity maintenance where we derive the gradients of our weighted graph edge weights required for computing the control. Finally, we perform multiple simulations to validate the connectivity maintenance performance of our DCMU algorithm under robot motion and sensing uncertainties and show an improvement compared to previous work.},
  language = {en}
}


@incollection{Oliehoek2012,
  title = {Decentralized {{POMDPs}}},
  booktitle = {Reinforcement {{Learning}}},
  author = {Oliehoek, Frans A.},
  editor = {Wiering, Marco and {van Otterlo}, Martijn},
  year = {2012},
  volume = {12},
  pages = {471--503},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-27645-3_15},
  url = {http://link.springer.com/10.1007/978-3-642-27645-3_15},
  urldate = {2021-09-29},
  abstract = {This chapter presents an overview of the decentralized POMDP (DecPOMDP) framework. In a Dec-POMDP, a team of agents collaborates to maximize a global reward based on local information only. This means that agents do not observe a Markovian signal during execution and therefore the agents' individual policies map from histories to actions. Searching for an optimal joint policy is an extremely hard problem: it is NEXP-complete. This suggests, assuming NEXP=EXP, that any optimal solution method will require doubly exponential time in the worst case. This chapter focuses on planning for Dec-POMDPs over a finite horizon. It covers the forward heuristic search approach to solving Dec-POMDPs, as well as the backward dynamic programming approach. Also, it discusses how these relate to the optimal Q-value function of a Dec-POMDP. Finally, it provides pointers to other solution methods and further related topics.},
  isbn = {978-3-642-27644-6 978-3-642-27645-3},
  language = {en}
}



